{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dgs-wqMsFMv"
      },
      "outputs": [],
      "source": [
        "%pip install transformers datasets peft\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8YeA0pXsKVa"
      },
      "outputs": [],
      "source": [
        "\n",
        "%pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-X-pZGZsKTT"
      },
      "outputs": [],
      "source": [
        "\n",
        "from datasets import Dataset, load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"ArtifactAI/arxiv-physics-instruct-tune-30k\")\n",
        "\n",
        "# Convert the 'train' split to a DataFrame\n",
        "df = dataset['train'].to_pandas()\n",
        "\n",
        "# Structure the data\n",
        "def structure_data(row):\n",
        "    question = row['question']\n",
        "    answer = row['answer']\n",
        "    formatted_text = f\"Can you answer this question: {question}? The answer is answer.\"\n",
        "    return {\n",
        "        \"text\": formatted_text,  # Store as string\n",
        "        \"label\": {\n",
        "            \"prompt\": formatted_text,  # Store as string\n",
        "            \"completion\": answer  # Store as string\n",
        "        }\n",
        "    }\n",
        "\n",
        "structured_data = df.apply(structure_data, axis=1).tolist()\n",
        "dataset_dict = Dataset.from_dict({\"data\": structured_data})\n",
        "\n",
        "# Split the dataset: 80% train, 20% val+test\n",
        "train_val_test_split = dataset_dict.train_test_split(test_size=0.2)\n",
        "train_data = train_val_test_split['train']\n",
        "\n",
        "# Split the 20% into 10% validation and 10% test\n",
        "val_test_split = train_val_test_split['test'].train_test_split(test_size=0.5)\n",
        "val_data = val_test_split['train']\n",
        "test_data = val_test_split['test']\n",
        "\n",
        "# Function to check data integrity\n",
        "def check_data_integrity(data):\n",
        "    for idx, item in enumerate(data):\n",
        "        if not isinstance(item['text'], str):\n",
        "            print(f\"Error in text at index {idx}: {item['text']}\")\n",
        "        if not isinstance(item['label']['prompt'], str):\n",
        "            print(f\"Error in prompt at index {idx}: {item['label']['prompt']}\")\n",
        "        if not isinstance(item['label']['completion'], str):\n",
        "            print(f\"Error in completion at index {idx}: {item['label']['completion']}\")\n",
        "\n",
        "check_data_integrity(structured_data)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyZy29q2sOaa"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tiktoken\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "# Define the tokenization function\n",
        "def tokenize_function(examples):\n",
        "    texts = examples['data']\n",
        "    input_texts = [ex['text'] for ex in texts]\n",
        "    prompts = [ex['label']['prompt'] for ex in texts]\n",
        "    completions = [ex['label']['completion'] for ex in texts]\n",
        "\n",
        "    # Tokenize inputs, prompts, and completions\n",
        "    def encode(text):\n",
        "        return tokenizer.encode(text)\n",
        "\n",
        "    input_ids = [encode(text) for text in input_texts]\n",
        "    prompt_ids = [encode(prompt) for prompt in prompts]\n",
        "    completion_ids = [encode(completion) for completion in completions]\n",
        "\n",
        "    # Handle padding and truncation\n",
        "    max_length = 680\n",
        "    pad_token_id = tokenizer.encode('[PAD]')[0] if tokenizer.encode('[PAD]') else tokenizer.eos_token_id\n",
        "\n",
        "    def pad_and_truncate(ids):\n",
        "        if len(ids) > max_length:\n",
        "            return ids[:max_length]\n",
        "        else:\n",
        "            return ids + [pad_token_id] * (max_length - len(ids))\n",
        "\n",
        "    padded_input_ids = [pad_and_truncate(ids) for ids in input_ids]\n",
        "    padded_prompt_ids = [pad_and_truncate(ids) for ids in prompt_ids]\n",
        "    padded_completion_ids = [pad_and_truncate(ids) for ids in completion_ids]\n",
        "\n",
        "    # Generate attention masks\n",
        "    attention_masks = [[1] * len(ids) + [0] * (max_length - len(ids)) for ids in padded_input_ids]\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": padded_input_ids,\n",
        "        \"prompt_ids\": padded_prompt_ids,\n",
        "        \"completion_ids\": padded_completion_ids,\n",
        "        \"attention_mask\": attention_masks\n",
        "    }\n",
        "\n",
        "# Apply the tokenization function to the dataset\n",
        "tokenized_train_data = train_data.map(tokenize_function, batched=True)\n",
        "tokenized_val_data = val_data.map(tokenize_function, batched=True)\n",
        "tokenized_test_data = test_data.map(tokenize_function, batched=True)\n",
        "\n",
        "# Convert to PyTorch Dataset\n",
        "import torch\n",
        "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
        "\n",
        "class CustomDataset(TorchDataset):\n",
        "    def __init__(self, tokenized_data):\n",
        "        self.input_ids = tokenized_data['input_ids']\n",
        "        self.attention_mask = tokenized_data['attention_mask']\n",
        "        self.labels = tokenized_data['completion_ids']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "train_dataset = CustomDataset(tokenized_train_data)\n",
        "val_dataset = CustomDataset(tokenized_val_data)\n",
        "test_dataset = CustomDataset(tokenized_test_data)\n",
        "\n",
        "\n",
        "# Define DataLoader for batching\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
        "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
        "    labels = torch.stack([item['labels'] for item in batch])\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iyawKdT_tF-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZiwvmLXsOYi"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# Load GPT-2 Model and Tokenizer\n",
        "model_name_or_path = 'gpt2-medium'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "# Configure LoRA Parameters with reduced rank\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,  # Task type: causal language modeling\n",
        "    inference_mode=False,          # Set to False for training mode\n",
        "    r=3,                           # Reduced rank (previously 8)\n",
        "    lora_alpha=16,                 # Scaled alpha (previously 32)\n",
        "    lora_dropout=0.1,              # Dropout rate\n",
        "    fan_in_fan_out=False           # Change parameter sharing method\n",
        ")\n",
        "\n",
        "# Apply PEFT with LoRA to the model\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Print the number of trainable parameters\n",
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpDIoadmsOWb"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import torch\n",
        "\n",
        "# Define training parameters\n",
        "lr = 2e-4  # Starting learning rate\n",
        "num_epochs = 3\n",
        "warmup_steps = 250 # Adjust based on dataset size and model complexity\n",
        "\n",
        "# Optimizer and learning rate scheduler setup\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "lr_scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps,\n",
        ")\n",
        "\n",
        "# Optionally, add gradient clipping\n",
        "max_grad_norm = 1.0  # Adjust as needed\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBOOh7udsOUE"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "# Define checkpointing functions\n",
        "def save_checkpoint(model, optimizer, epoch, loss, checkpoint_dir):\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pt')\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "    }, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer, device):\n",
        "    if os.path.isfile(checkpoint_path):\n",
        "        print(f\"Loading checkpoint '{checkpoint_path}'\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        loss = checkpoint.get('loss', None)\n",
        "        print(f\"Loaded checkpoint '{checkpoint_path}' (epoch {start_epoch})\")\n",
        "        return start_epoch, loss\n",
        "    else:\n",
        "        print(f\"No checkpoint found at '{checkpoint_path}', starting from scratch\")\n",
        "        return 0, None\n",
        "\n",
        "# Device Setup - Efficient assignment based on availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Move model to the correct device (already assumed to be done)\n",
        "model.to(device)\n",
        "\n",
        "# Checkpoint Path Definition - Standard path construction\n",
        "checkpoint_dir = '/content/drive/MyDrive/checkpointsgpt2'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "latest_checkpoint = os.path.join(checkpoint_dir, 'checkpoint_latest_epoch_2.pt')\n",
        "\n",
        "# Loading Call - Simple and clear\n",
        "start_epoch, _ = load_checkpoint(latest_checkpoint, model, optimizer, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "esU30KNDstqH"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from transformers import AdamW as TransformersAdamW  # Import specifically for Transformers\n",
        "from tqdm import tqdm  # Import tqdm for progress bar\n",
        "import os\n",
        "\n",
        "# Initialize the GradScaler\n",
        "scaler = GradScaler()\n",
        "accumulation_steps = 4\n",
        "# Training loop with validation and checkpointing\n",
        "best_val_loss = float('inf')  # Initialize with a very large value\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    total_loss = 0.0\n",
        "    model.train()\n",
        "\n",
        "    # Initialize tqdm with train_dataloader for progress bar\n",
        "    progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}', unit=' batches')\n",
        "\n",
        "    # Training phase\n",
        "    for step, batch in enumerate(progress_bar):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (step + 1) % accumulation_steps == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Update tqdm progress bar\n",
        "        progress_bar.set_postfix({'training_loss': total_loss / (step + 1)})\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}: Average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Save checkpoint at the end of each epoch\n",
        "    save_checkpoint(model, optimizer, epoch, avg_train_loss, checkpoint_dir)\n",
        "\n",
        "    # Save the latest checkpoint after each epoch\n",
        "    latest_checkpoint_epoch = os.path.join(checkpoint_dir, f'checkpoint_latest_epoch_{epoch}.pt')\n",
        "    torch.save({\n",
        "        'epoch': epoch + 1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': avg_train_loss\n",
        "    }, latest_checkpoint_epoch)\n",
        "\n",
        "print(\"Training complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkBPbaW8Cziu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Define the path to save the model\n",
        "model_save_path = '/content/drive/MyDrive/checkpointsgpt2'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(model_save_path):\n",
        "    os.makedirs(model_save_path)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "torch.save(model.state_dict(), os.path.join(model_save_path, 'finetuned_model2.pth'))\n",
        "\n",
        "print(\"Model saved to Google Drive!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# Define the path to the saved model\n",
        "model_save_path = '/content/drive/MyDrive/checkpointsgpt2'\n",
        "model_file = 'finetuned_model.pth'\n",
        "\n",
        "# Load the saved model\n",
        "def load_saved_model(save_path, model_file):\n",
        "    \"\"\"\n",
        "    Loads the saved model from the specified path.\n",
        "\n",
        "    Args:\n",
        "        save_path (str): The path to the saved model.\n",
        "        model_file (str): The file name of the saved model.\n",
        "\n",
        "    Returns:\n",
        "        torch.nn.Module: The loaded model.\n",
        "    \"\"\"\n",
        "    # Create a new instance of the LoRA model\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,  # Task type: causal language modeling\n",
        "        inference_mode=False,          # Set to False for training mode\n",
        "        r=3,                           # Reduced rank (previously 8)\n",
        "        lora_alpha=16,                 # Scaled alpha (previously 32)\n",
        "        lora_dropout=0.1,              # Dropout rate\n",
        "        fan_in_fan_out=False           # Change parameter sharing method\n",
        "    )\n",
        "    model = get_peft_model(AutoModelForCausalLM.from_pretrained('gpt2-medium'), peft_config)\n",
        "\n",
        "    # Load the saved model state dict\n",
        "    state_dict = torch.load(os.path.join(save_path, model_file), map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "\n",
        "    # Load the state dict into the model\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    return model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Load the saved model\n",
        "loaded_model = load_saved_model(model_save_path, model_file).to(device)\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "FCNaD9abhVWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_metric\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize the ROUGE metric\n",
        "rouge = load_metric(\"rouge\")\n",
        "# Define device  <-- Add this line\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2-medium')\n",
        "# Evaluation\n",
        "predictions = []\n",
        "references = []\n",
        "total_loss = 0.0\n",
        "num_batches = len(test_dataloader)\n",
        "\n",
        "with torch.no_grad():\n",
        "    with tqdm(total=num_batches, desc=\"Evaluating\") as pbar:\n",
        "        for batch in test_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Generate outputs\n",
        "            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=50, num_beams=4)\n",
        "            decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "            # Append to lists\n",
        "            predictions.extend(decoded_preds)\n",
        "            references.extend(decoded_labels)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Update tqdm progress bar with loss\n",
        "            pbar.set_postfix({'Batch Loss': loss.item()})\n",
        "            pbar.update(1)\n",
        "\n",
        "# Compute ROUGE scores\n",
        "results = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
        "\n",
        "# Extract relevant scores\n",
        "rouge1_precision = results[\"rouge1\"].high.precision\n",
        "rouge1_recall = results[\"rouge1\"].high.recall\n",
        "rouge1_f1 = results[\"rouge1\"].high.fmeasure\n",
        "\n",
        "rouge2_precision = results[\"rouge2\"].high.precision\n",
        "rouge2_recall = results[\"rouge2\"].high.recall\n",
        "rouge2_f1 = results[\"rouge2\"].high.fmeasure\n",
        "\n",
        "rougel_precision = results[\"rougeL\"].high.precision\n",
        "rougel_recall = results[\"rougeL\"].high.recall\n",
        "rougel_f1 = results[\"rougeL\"].high.fmeasure\n",
        "\n",
        "# Calculate accuracy as an additional metric\n",
        "correct = 0\n",
        "total = 0\n",
        "for pred, label in zip(predictions, references):\n",
        "    if pred.strip() == label.strip():\n",
        "        correct += 1\n",
        "    total += 1\n",
        "accuracy = correct / total\n",
        "\n",
        "# Calculate partial accuracy as an additional metric\n",
        "partial_correct = 0\n",
        "partial_total = 0\n",
        "for pred, label in zip(predictions, references):\n",
        "    pred_words = pred.split()\n",
        "    label_words = label.split()\n",
        "    common_words = set(pred_words) & set(label_words)\n",
        "    partial_correct += len(common_words)\n",
        "    partial_total += len(label_words)\n",
        "partial_accuracy = partial_correct / partial_total\n",
        "\n",
        "# Print results\n",
        "print(f\"Total Evaluation Loss: {total_loss / num_batches:.4f}\")\n",
        "print(f\"ROUGE-1 Precision: {rouge1_precision:.4f}, Recall: {rouge1_recall:.4f}, F1: {rouge1_f1:.4f}\")\n",
        "print(f\"ROUGE-2 Precision: {rouge2_precision:.4f}, Recall: {rouge2_recall:.4f}, F1: {rouge2_f1:.4f}\")\n",
        "print(f\"ROUGE-L Precision: {rougel_precision:.4f}, Recall: {rougel_recall:.4f}, F1: {rougel_f1:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Partial Accuracy: {partial_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "waM9SohhhVTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install evaluate"
      ],
      "metadata": {
        "id": "wPuQpakqhVRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import evaluate\n",
        "# Print results\n",
        "print(f\"Total Evaluation Loss: {total_loss / num_batches:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Partial Accuracy: {partial_accuracy:.4f}\")\n",
        "\n",
        "# Compute BLEU scores\n",
        "bleu_metric = evaluate.load(\"bleu\")\n",
        "bleu_results = bleu_metric.compute(predictions=predictions, references=references)\n",
        "\n",
        "# Extract relevant scores\n",
        "bleu_score = bleu_results[\"bleu\"]\n",
        "\n",
        "print(f\"BLEU Score: {bleu_score:.4f}\")"
      ],
      "metadata": {
        "id": "It3_yOVDhVO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from evaluate import load\n",
        "\n",
        "# ...\n",
        "\n",
        "# Compute METEOR scores\n",
        "meteor_metric = load(\"meteor\")\n",
        "meteor_results = meteor_metric.compute(predictions=predictions, references=references)\n",
        "\n",
        "# Extract relevant scores\n",
        "meteor_score = meteor_results[\"meteor\"]\n",
        "\n",
        "print(f\"METEOR Score: {meteor_score:.4f}\")"
      ],
      "metadata": {
        "id": "RBu1XFdVhVM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5IORrmHZwk09"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
