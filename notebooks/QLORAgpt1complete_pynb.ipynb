{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers datasets peft\n"
      ],
      "metadata": {
        "id": "J_Oh0h3s_b3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%pip install rake-nltk\n"
      ],
      "metadata": {
        "id": "1_Z2SqUjfpU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%pip install tiktoken"
      ],
      "metadata": {
        "id": "cRiKDI85_cWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%pip install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNqEIOuA0myl",
        "outputId": "29780cf6-f2eb-4d4d-b3f8-f459bbc6fed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QCrh_og-t3C"
      },
      "outputs": [],
      "source": [
        "\n",
        "from datasets import Dataset, load_dataset\n",
        "# HUGGING_FACE_TOKEN_VALUE=(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)\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"vinhtran2611/ArtifactAI_arxiv-physics-instruct-tune-30k_formated\")\n",
        "\n",
        "# Convert the 'train' split to a DataFrame\n",
        "df = dataset['train'].to_pandas()\n",
        "\n",
        "# Structure the data\n",
        "def structure_data(row):\n",
        "    question = row['instruction']\n",
        "    answer = row['output']\n",
        "    formatted_text = f\"Can you answer this question: {question}? The answer is answer.\"\n",
        "    return {\n",
        "        \"text\": formatted_text,  # Store as string\n",
        "        \"label\": {\n",
        "            \"prompt\": formatted_text,  # Store as string\n",
        "            \"completion\": answer  # Store as string\n",
        "        }\n",
        "    }\n",
        "\n",
        "structured_data = df.apply(structure_data, axis=1).tolist()\n",
        "dataset_dict = Dataset.from_dict({\"data\": structured_data})\n",
        "\n",
        "# Split the dataset: 80% train, 20% val+test\n",
        "train_val_test_split = dataset_dict.train_test_split(test_size=0.2)\n",
        "train_data = train_val_test_split['train']\n",
        "\n",
        "# Split the 20% into 10% validation and 10% test\n",
        "val_test_split = train_val_test_split['test'].train_test_split(test_size=0.5)\n",
        "val_data = val_test_split['train']\n",
        "test_data = val_test_split['test']\n",
        "\n",
        "# Function to check data integrity\n",
        "def check_data_integrity(data):\n",
        "    for idx, item in enumerate(data):\n",
        "        if not isinstance(item['text'], str):\n",
        "            print(f\"Error in text at index {idx}: {item['text']}\")\n",
        "        if not isinstance(item['label']['prompt'], str):\n",
        "            print(f\"Error in prompt at index {idx}: {item['label']['prompt']}\")\n",
        "        if not isinstance(item['label']['completion'], str):\n",
        "            print(f\"Error in completion at index {idx}: {item['label']['completion']}\")\n",
        "\n",
        "check_data_integrity(structured_data)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tiktoken\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "# Define the tokenization function\n",
        "def tokenize_function(examples):\n",
        "    texts = examples['data']\n",
        "    input_texts = [ex['text'] for ex in texts]\n",
        "    prompts = [ex['label']['prompt'] for ex in texts]\n",
        "    completions = [ex['label']['completion'] for ex in texts]\n",
        "\n",
        "    # Tokenize inputs, prompts, and completions\n",
        "    def encode(text):\n",
        "        return tokenizer.encode(text)\n",
        "\n",
        "    input_ids = [encode(text) for text in input_texts]\n",
        "    prompt_ids = [encode(prompt) for prompt in prompts]\n",
        "    completion_ids = [encode(completion) for completion in completions]\n",
        "\n",
        "    # Handle padding and truncation\n",
        "    max_length = 680 # Adjust as needed\n",
        "    pad_token_id = tokenizer.encode('[PAD]')[0] if tokenizer.encode('[PAD]') else tokenizer.eos_token_id\n",
        "\n",
        "    def pad_and_truncate(ids):\n",
        "        if len(ids) > max_length:\n",
        "            return ids[:max_length]\n",
        "        else:\n",
        "            return ids + [pad_token_id] * (max_length - len(ids))\n",
        "\n",
        "    padded_input_ids = [pad_and_truncate(ids) for ids in input_ids]\n",
        "    padded_prompt_ids = [pad_and_truncate(ids) for ids in prompt_ids]\n",
        "    padded_completion_ids = [pad_and_truncate(ids) for ids in completion_ids]\n",
        "\n",
        "    # Generate attention masks\n",
        "    attention_masks = [[1] * len(ids) + [0] * (max_length - len(ids)) for ids in padded_input_ids]\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": padded_input_ids,\n",
        "        \"prompt_ids\": padded_prompt_ids,\n",
        "        \"completion_ids\": padded_completion_ids,\n",
        "        \"attention_mask\": attention_masks\n",
        "    }\n",
        "\n",
        "# Apply the tokenization function to the dataset\n",
        "tokenized_train_data = train_data.map(tokenize_function, batched=True)\n",
        "tokenized_val_data = val_data.map(tokenize_function, batched=True)\n",
        "tokenized_test_data = test_data.map(tokenize_function, batched=True)\n",
        "\n",
        "# Convert to PyTorch Dataset\n",
        "import torch\n",
        "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
        "\n",
        "class CustomDataset(TorchDataset):\n",
        "    def __init__(self, tokenized_data):\n",
        "        self.input_ids = tokenized_data['input_ids']\n",
        "        self.attention_mask = tokenized_data['attention_mask']\n",
        "        self.labels = tokenized_data['completion_ids']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "train_dataset = CustomDataset(tokenized_train_data)\n",
        "val_dataset = CustomDataset(tokenized_val_data)\n",
        "test_dataset = CustomDataset(tokenized_test_data)\n",
        "\n",
        "\n",
        "# Define DataLoader for batching\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
        "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
        "    labels = torch.stack([item['labels'] for item in batch])\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=7, shuffle=True, collate_fn=collate_fn)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=7, shuffle=False, collate_fn=collate_fn)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=7, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "XbLCM7IJfC9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "import torch\n",
        "# Load GPT-2 Model and Tokenizer\n",
        "model_name_or_path = 'gpt2-medium'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
      ],
      "metadata": {
        "id": "QqYJgzDxoK6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "import torch\n",
        "\n",
        "# Step 1: Load the pre-trained GPT-2 model with FP16 precision\n",
        "model_name_or_path = 'gpt2-medium'\n",
        "device_map = \"auto\"  # Automatically distribute model across devices if necessary\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    device_map=device_map,           # Distribute model across devices\n",
        "    torch_dtype=torch.float16       # Use 16-bit precision (FP16)\n",
        ")\n",
        "\n",
        "# # Optional: Load the tokenizer if you need to preprocess text\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "# Step 2: Configure LoRA parameters\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,      # Task type: causal language modeling\n",
        "    inference_mode=False,              # Set to False for training mode\n",
        "    r=64,                              # Rank of the low-rank update matrices\n",
        "    lora_alpha=128,                     # Scaling factor for LoRA\n",
        "    lora_dropout=0.6,                  # Dropout rate\n",
        "    fan_in_fan_out=False               # Control how matrices are shared\n",
        ")\n",
        "\n",
        "# Step 3: Apply LoRA to the model\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Step 4: Print the number of trainable parameters in the model\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEn_wIWK_Nyj",
        "outputId": "45a82f8b-fead-4d04-d319-a2b11e7e2d56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 6,291,456 || all params: 361,114,624 || trainable%: 1.7422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import torch\n",
        "\n",
        "# Define training parameters\n",
        "lr = 1e-4  # Starting learning rate, could experiment with 1e-4 if needed\n",
        "num_epochs = 3\n",
        "warmup_steps = 250 # Adjust based on dataset size\n",
        "\n",
        "# Ensure optimizer only updates trainable parameters (LoRA and any unfrozen layers)\n",
        "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "\n",
        "# Calculate total training steps\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "\n",
        "# Learning rate scheduler setup\n",
        "lr_scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps,\n",
        ")\n",
        "\n",
        "# Optionally, add gradient clipping\n",
        "max_grad_norm = 1.0  # Adjust as needed\n",
        "torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), max_grad_norm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kA38KnGh_Nv3",
        "outputId": "5613aba0-59e9-4dbf-e1eb-c12c8510237c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define checkpointing functions\n",
        "def save_checkpoint(model, optimizer, epoch, loss, checkpoint_dir):\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pt')\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'lora_state_dict': model.peft_model.state_dict(),  # Save only LoRA adapter state dict\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "    }, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer, device):\n",
        "    if os.path.isfile(checkpoint_path):\n",
        "        print(f\"Loading checkpoint '{checkpoint_path}'\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        model.peft_model.load_state_dict(checkpoint['lora_state_dict'])  # Load LoRA adapter weights\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        loss = checkpoint.get('loss', None)\n",
        "        print(f\"Loaded checkpoint '{checkpoint_path}' (epoch {start_epoch})\")\n",
        "        return start_epoch, loss\n",
        "    else:\n",
        "        print(f\"No checkpoint found at '{checkpoint_path}', starting from scratch\")\n",
        "        return 0, None\n",
        "\n",
        "# Device Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)  # Move the entire model to the GPU\n",
        "# Checkpoint Path Definition\n",
        "checkpoint_dir = '/content/drive/MyDrive/checkpointsgpt2'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "latest_checkpoint = os.path.join(checkpoint_dir, 'checkpoint_latest_epoch_2.pt')\n",
        "\n",
        "# Loading checkpoint (LoRA only)\n",
        "start_epoch, _ = load_checkpoint(latest_checkpoint, model, optimizer, device)"
      ],
      "metadata": {
        "id": "aMt5aUFE_UzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tqdm import tqdm  # Import tqdm for progress bar\n",
        "\n",
        "# Training loop\n",
        "accumulation_steps = 4\n",
        "best_val_loss = float('inf')  # Initialize with a very large value\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    total_loss = 0.0\n",
        "    model.train()\n",
        "\n",
        "    # Initialize tqdm with train_dataloader for progress bar\n",
        "    progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}', unit=' batches')\n",
        "\n",
        "    # Training phase\n",
        "    for step, batch in enumerate(progress_bar):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient accumulation handling\n",
        "        if (step + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Update tqdm progress bar\n",
        "        progress_bar.set_postfix({'training_loss': total_loss / (step + 1)})\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}: Average training loss: {avg_train_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "lrPwiKey1CO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Define the path to save the model\n",
        "model_save_path = '/content/drive/MyDrive/checkpointsgpt2'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(model_save_path):\n",
        "    os.makedirs(model_save_path)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "torch.save(model.state_dict(), os.path.join(model_save_path, 'QLORAfinetuned_model1.pth'))\n",
        "\n",
        "print(\"Model saved to Google Drive!\")"
      ],
      "metadata": {
        "id": "1jT4qm5vchm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# Define the path to the saved model\n",
        "model_save_path = '/content/drive/MyDrive/checkpointsgpt2'\n",
        "model_file = 'QLORAfinetuned_model.pth'\n",
        "\n",
        "# Load the saved model\n",
        "def load_saved_model(save_path, model_file):\n",
        "    \"\"\"\n",
        "    Loads the saved model from the specified path.\n",
        "\n",
        "    Args:\n",
        "        save_path (str): The path to the saved model.\n",
        "        model_file (str): The file name of the saved model.\n",
        "\n",
        "    Returns:\n",
        "        torch.nn.Module: The loaded model.\n",
        "    \"\"\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    device_map=device_map,           # Distribute model across devices\n",
        "    torch_dtype=torch.float16       # Use 16-bit precision (FP16)\n",
        "    )\n",
        "\n",
        "    # # Optional: Load the tokenizer if you need to preprocess text\n",
        "    # tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "    # Step 2: Configure LoRA parameters\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,      # Task type: causal language modeling\n",
        "        inference_mode=False,              # Set to False for training mode\n",
        "        r=64,                              # Rank of the low-rank update matrices\n",
        "       lora_alpha=128,                     # Scaling factor for LoRA\n",
        "       lora_dropout=0.6,                  # Dropout rate\n",
        "       fan_in_fan_out=False               # Control how matrices are shared\n",
        "    )\n",
        "    model = get_peft_model(AutoModelForCausalLM.from_pretrained('gpt2-medium'), peft_config)\n",
        "\n",
        "    # Load the saved model state dict\n",
        "    state_dict = torch.load(os.path.join(save_path, model_file), map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "\n",
        "    # Load the state dict into the model\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    return model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Load the saved model\n",
        "loaded_model = load_saved_model(model_save_path, model_file).to(device)\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "ZPD6U3mrckTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_metric\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize the ROUGE metric\n",
        "rouge = load_metric(\"rouge\")\n",
        "# Define device  <-- Add this line\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2-medium')\n",
        "# Evaluation\n",
        "predictions = []\n",
        "references = []\n",
        "total_loss = 0.0\n",
        "num_batches = len(test_dataloader)\n",
        "\n",
        "with torch.no_grad():\n",
        "    with tqdm(total=num_batches, desc=\"Evaluating\") as pbar:\n",
        "        for batch in test_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Generate outputs\n",
        "            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=50, num_beams=4)\n",
        "            decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "            # Append to lists\n",
        "            predictions.extend(decoded_preds)\n",
        "            references.extend(decoded_labels)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Update tqdm progress bar with loss\n",
        "            pbar.set_postfix({'Batch Loss': loss.item()})\n",
        "            pbar.update(1)\n",
        "\n",
        "# Compute ROUGE scores\n",
        "results = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
        "\n",
        "# Extract relevant scores\n",
        "rouge1_precision = results[\"rouge1\"].high.precision\n",
        "rouge1_recall = results[\"rouge1\"].high.recall\n",
        "rouge1_f1 = results[\"rouge1\"].high.fmeasure\n",
        "\n",
        "rouge2_precision = results[\"rouge2\"].high.precision\n",
        "rouge2_recall = results[\"rouge2\"].high.recall\n",
        "rouge2_f1 = results[\"rouge2\"].high.fmeasure\n",
        "\n",
        "rougel_precision = results[\"rougeL\"].high.precision\n",
        "rougel_recall = results[\"rougeL\"].high.recall\n",
        "rougel_f1 = results[\"rougeL\"].high.fmeasure\n",
        "\n",
        "# Calculate accuracy as an additional metric\n",
        "correct = 0\n",
        "total = 0\n",
        "for pred, label in zip(predictions, references):\n",
        "    if pred.strip() == label.strip():\n",
        "        correct += 1\n",
        "    total += 1\n",
        "accuracy = correct / total\n",
        "\n",
        "# Calculate partial accuracy as an additional metric\n",
        "partial_correct = 0\n",
        "partial_total = 0\n",
        "for pred, label in zip(predictions, references):\n",
        "    pred_words = pred.split()\n",
        "    label_words = label.split()\n",
        "    common_words = set(pred_words) & set(label_words)\n",
        "    partial_correct += len(common_words)\n",
        "    partial_total += len(label_words)\n",
        "partial_accuracy = partial_correct / partial_total\n",
        "\n",
        "# Print results\n",
        "print(f\"Total Evaluation Loss: {total_loss / num_batches:.4f}\")\n",
        "print(f\"ROUGE-1 Precision: {rouge1_precision:.4f}, Recall: {rouge1_recall:.4f}, F1: {rouge1_f1:.4f}\")\n",
        "print(f\"ROUGE-2 Precision: {rouge2_precision:.4f}, Recall: {rouge2_recall:.4f}, F1: {rouge2_f1:.4f}\")\n",
        "print(f\"ROUGE-L Precision: {rougel_precision:.4f}, Recall: {rougel_recall:.4f}, F1: {rougel_f1:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Partial Accuracy: {partial_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "GPj2gk3fRWP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install rouge_score # install the rouge_score module"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "x3MFtce_macV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install evaluate"
      ],
      "metadata": {
        "id": "_EA-Xz2aRd2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import evaluate\n",
        "# Print results\n",
        "print(f\"Total Evaluation Loss: {total_loss / num_batches:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Partial Accuracy: {partial_accuracy:.4f}\")\n",
        "\n",
        "# Compute BLEU scores\n",
        "bleu_metric = evaluate.load(\"bleu\")\n",
        "bleu_results = bleu_metric.compute(predictions=predictions, references=references)\n",
        "\n",
        "# Extract relevant scores\n",
        "bleu_score = bleu_results[\"bleu\"]\n",
        "\n",
        "print(f\"BLEU Score: {bleu_score:.4f}\")"
      ],
      "metadata": {
        "id": "Anw_q5L9Re_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install rouge_score"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "q3bxDg9hjRhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from evaluate import load\n",
        "\n",
        "# ...\n",
        "\n",
        "# Compute METEOR scores\n",
        "meteor_metric = load(\"meteor\")\n",
        "meteor_results = meteor_metric.compute(predictions=predictions, references=references)\n",
        "\n",
        "# Extract relevant scores\n",
        "meteor_score = meteor_results[\"meteor\"]\n",
        "\n",
        "print(f\"METEOR Score: {meteor_score:.4f}\")"
      ],
      "metadata": {
        "id": "0BKAnT6GRiqr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
